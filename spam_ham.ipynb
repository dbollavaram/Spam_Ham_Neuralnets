{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import glob\n",
    "import re\n",
    "import scipy.io\n",
    "import codecs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import neural_network\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n",
    "\n",
    "BASE_DIR = 'C:/Users/Deepika_Bollavaram/Documents/IEOR290/Project/data/data/'\n",
    "SPAM_DIR = 'spam/'\n",
    "HAM_DIR = 'ham/'\n",
    "TEST_DIR = 'test/'\n",
    "\n",
    "def generate_text_list(filenames):\n",
    "    text_list = []\n",
    "    for fname in filenames:\n",
    "        with codecs.open(fname, \"r\", \"utf-8\", errors = 'ignore') as f:\n",
    "            text = f.read().replace('\\r\\n', ' ')\n",
    "            #text = text.encode('utf-8')\n",
    "            #text = ' '.join(str(r) for v in spam_list for r in v)\n",
    "            text_list.append(text)\n",
    "    return text_list\n",
    "\n",
    "#reading files \n",
    "spam_filenames = glob.glob(BASE_DIR + SPAM_DIR + '*.txt')\n",
    "spam_list = generate_text_list(spam_filenames)\n",
    "\n",
    "ham_filenames = glob.glob(BASE_DIR + HAM_DIR + '*.txt')\n",
    "ham_list = generate_text_list(ham_filenames)\n",
    "\n",
    "test_filenames = [BASE_DIR + TEST_DIR + str(x) + '.txt' for x in range(5857)]\n",
    "test_list = generate_text_list(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list = ham_list + spam_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lemmatizing\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "#Parts-of-speech tagging the text and selecting only categories J, V, N and R\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''Treebank to wordnet POS tag'''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n' #basecase POS\n",
    "\n",
    "train_list_clean=[]\n",
    "\n",
    "for z in range(0,len(train_list)):\n",
    "    train_list_split = train_list[z].split()\n",
    "    toks = pos_tag(train_list_split)\n",
    "    train_list_wnl = ' '.join([wnl.lemmatize(w,pos=get_wordnet_pos(t)) for w,t in toks])\n",
    "    train_list_clean.append(train_list_wnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Training and Testing Split\n",
    "y_ham = [0] * len(ham_list)\n",
    "y_spam = [1] * len(spam_list)\n",
    "y = y_ham + y_spam\n",
    "text_train, text_test, y_train, y_test = train_test_split(train_list_clean,y, test_size=0.25, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Method 1\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(text_train)\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "#using tf-idf scores for prediction\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "print(X_train_tf.shape)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "\n",
    "#building neural network\n",
    "classifier = neural_network.MLPClassifier(activation = 'relu', hidden_layer_sizes = 200, batch_size = 100, early_stopping = True, validation_fraction = 0.2)\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "X_new_counts = count_vect.transform(text_test)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "#predicting on test set\n",
    "predicted = classifier.predict(X_new_tfidf)\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5172, 50463)\n",
      "(5172, 50463)\n",
      "(5172, 50463)\n",
      "[[3663    9]\n",
      " [   2 1498]]\n"
     ]
    }
   ],
   "source": [
    "### Building on entire data set\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_list)\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "print(X_train_tf.shape)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "\n",
    "classifier = neural_network.MLPClassifier(activation = 'relu', hidden_layer_sizes = 200, batch_size = 100, early_stopping = True, validation_fraction = 0.2)\n",
    "classifier.fit(X_train_tfidf, y)\n",
    "#X_new_counts = count_vect.transform(text_test)\n",
    "#X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = classifier.predict(X_train_tfidf)\n",
    "print(confusion_matrix(y, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008507347254447023"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.count(1)\n",
    "float(11)/float(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9978731631863882"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(5161)/float(len(train_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Kaggle_new_counts = count_vect.transform(test_list)\n",
    "Kaggle_new_tfidf = tfidf_transformer.transform(Kaggle_new_counts)\n",
    "predicted = classifier.predict(Kaggle_new_tfidf)\n",
    "kaggle = pd.DataFrame(predicted)\n",
    "kaggle.columns = ['Category']\n",
    "kaggle.to_csv(\"C:/Users/Deepika_Bollavaram/Documents/IEOR290/Project/Submissions/NN_May2_New_v3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
